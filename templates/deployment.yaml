apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "llm-service.fullname" . }}
  labels:
    {{- include "llm-service.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  {{- if or .Values.updateStrategy.type .Values.updateStrategy.rollingUpdate }}
  strategy: {{ .Values.updateStrategy | toYaml | nindent 4 }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "llm-service.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "llm-service.labels" . | nindent 8 }}
        {{- with .Values.podLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
    spec:
      imagePullSecrets: {{ include "llm-service.fullname" . }}-regcred
      serviceAccountName: {{ include "llm-service.serviceAccountName" . }}
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag |  default (ternary (printf "%s-rocm" .Chart.AppVersion) (.Chart.AppVersion) (and (.Values.ollama.gpu.enabled) (eq .Values.ollama.gpu.type "amd"))) }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
          env:
            {{- if .Values.ollama.gpu.enabled (or (eq .Values.ollama.gpu.type "nvidia") (not .Values.ollama.gpu.type))}}
            - name: PATH
              value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
            - name: LD_LIBRARY_PATH
              value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: compute,utility
            - name: NVIDIA_VISIBLE_DEVICES
              value: all
            {{- end}}
          {{- if .Values.resources }}
          resources:
            {{- $limits := default dict .Values.resources.limits }}
            {{- if .Values.ollama.gpu.enabled }}
              {{- if or (eq .Values.ollama.gpu.type "nvidia") (not .Values.ollama.gpu.type) }}
                {{- $gpuLimit := dict (.Values.ollama.gpu.nvidiaResource | default "nvidia.com/gpu") (.Values.ollama.gpu.number | default 1) }}
                {{- $limits = merge $limits $gpuLimit }}
              {{- end }}
              {{- if eq .Values.ollama.gpu.type "amd" }}
                {{- $gpuLimit := dict "amd.com/gpu" (.Values.ollama.gpu.number | default 1) }}
                {{- $limits = merge $limits $gpuLimit }}
              {{- end }}
            {{- end }}
            {{- $ressources := deepCopy (dict "limits" $limits) | mergeOverwrite .Values.resources }}
            {{- toYaml $ressources | nindent 12 }}
          {{- end}}
          volumeMounts:
            - name: ollama-data
              mountPath: {{ .Values.ollama.mountPath | default "/root/.ollama" }}
          {{- with .Values.volumeMounts }}
            {{- toYaml . | nindent 12 }}
          {{- end }}
          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 6
            failureThreshold: 1
          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 6
            failureThreshold: 1
          {{- if or .Values.ollama.models .Values.ollama.defaultModel }}
          lifecycle:
            postStart:
              exec:
                command: [ "/bin/sh", "-c", "{{- printf "echo %s | xargs -n1 /bin/ollama pull %s" (include "llm-service.modelList" .) (ternary "--insecure" "" .Values.ollama.insecure)}}" ]
          {{- end }}
      volumes:
        - name: ollama-data
          {{- if .Values.persistentVolume.enabled }}
          persistentVolumeClaim:
            claimName: {{ .Values.persistentVolume.existingClaim |  default (printf "%s" (include "llm-service.fullname" .)) }}
          {{- else }}
          emptyDir: { }
          {{- end }}
        {{- with .Values.volumes }}
          {{- toYaml . | nindent 8 }}
        {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- if or .Values.ollama.gpu.enabled .Values.tolerations }}
      tolerations:
        {{- if and .Values.ollama.gpu.enabled (or (eq .Values.ollama.gpu.type "nvidia") (not .Values.ollama.gpu.type)) }}
        - key: "{{(.Values.ollama.gpu.nvidiaResource | default "nvidia.com/gpu")}}"
          operator: Exists
          effect: NoSchedule
        {{- end }}
        {{- with .Values.tolerations }}
          {{- toYaml . | nindent 8 }}
        {{- end }}
      {{- end }}